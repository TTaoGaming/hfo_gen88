{
  "gemma3:4b": {
    "simple_eval": {
      "accuracy": 60.0,
      "latency_ms": 3827.1924,
      "task": "basic"
    },
    "promptfoo": {
      "accuracy": 83.33333333333334,
      "pass": 5,
      "total": 6
    }
  },
  "qwen3:4b": {
    "simple_eval": {
      "accuracy": 100.0,
      "latency_ms": 6506.9712,
      "task": "basic"
    }
  },
  "gemma3:1b": {
    "simple_eval": {
      "accuracy": 100.0,
      "latency_ms": 3098.7394,
      "task": "basic"
    },
    "promptfoo": {
      "accuracy": 75.0,
      "pass": 9,
      "total": 12
    },
    "deepeval": {
      "accuracy": 66.66666666666666,
      "passed": 4,
      "total": 6
    },
    "llm_benchmark": {
      "tokens_per_sec": 65.19752817066383,
      "avg_latency_ms": 4527.267749998525
    }
  },
  "qwen3:0.6b": {
    "promptfoo": {
      "accuracy": 100.0,
      "pass": 6,
      "total": 6
    }
  },
  "phi3:mini": {
    "promptfoo": {
      "accuracy": 100.0,
      "pass": 6,
      "total": 6
    }
  }
}